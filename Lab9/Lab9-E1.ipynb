{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "631c1b24-4243-4948-8438-7f5c516af511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\azif\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\azif\\anaconda3\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\azif\\anaconda3\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\azif\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "422687e0-2ef2-4099-9270-ffa3452ada5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16de1dc6-24da-47be-83f1-efb1d9ceb0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf93459-1dff-4942-813b-19f13f9b764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Azif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Azif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Azif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6e6a065-6c52-4dd7-a844-c6e7157382c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "\"Rafael Nadal Joins Roger Federer in Missing U.S. Open\", \"Rafael Nadal Is Out of the Australian Open\",\n",
    "\"Biden Announces Virus Measures\",\n",
    "\"Biden's Virus Plans Meet Reality\",\n",
    "\"Where Biden's Virus Plan Stands\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4768807d-8e99-4442-bab8-dbc7b0237971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Rafael', 'Nadal', 'Joins', 'Roger', 'Federer', 'Missing', 'Open'], ['Rafael', 'Nadal', 'Is', 'Out', 'Australian', 'Open'], ['Biden', 'Announces', 'Virus', 'Measures'], ['Biden', 'Virus', 'Plans', 'Meet', 'Reality'], ['Where', 'Biden', 'Virus', 'Plan', 'Stands']]\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)  # Corrected: Now we're calling word_tokenize with the input text\n",
    "    tokens = [token for token in tokens if token.isalnum()]  # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize tokens\n",
    "    return tokens\n",
    "\n",
    "# Apply the preprocessing to each document\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
    "print(preprocessed_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abf92f04-8159-4ea1-b2d9-c9a9cd571937",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b04cd52e-06ed-4b9d-9b2f-a1fd9525060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus: bag-of-words representation of the documents\n",
    "#num_topics: number of topics to be extracted by the model\n",
    "#id2word=dictionary: dictionary mapping from word IDs to word\n",
    "#passes: number of passes through the corpus during training \n",
    "# Train an LDA model on the corpus with 4 topics using Gensimâ€™s LdaModel class\n",
    "lda_model = LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3dd1a32c-25d4-4109-a1a3-f12e539134d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_labels = []\n",
    "\n",
    "for i, doc in enumerate (preprocessed_documents):\n",
    "\tbow = dictionary.doc2bow(doc)\n",
    "\ttopics = lda_model.get_document_topics(bow)\n",
    "\tdominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "\tarticle_labels.append(dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9c5d7ca7-cbe1-4f49-a7d4-44b436ed7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e73aa27d-eb22-4bb6-b834-528d219b5312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic: \n",
      "                                             Article  Topic\n",
      "0  Rafael Nadal Joins Roger Federer in Missing U....      0\n",
      "1         Rafael Nadal Is Out of the Australian Open      1\n",
      "2                     Biden Announces Virus Measures      0\n",
      "3                   Biden's Virus Plans Meet Reality      1\n",
      "4                    Where Biden's Virus Plan Stands      0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'Article': documents, 'Topic': article_labels})\n",
    "\n",
    "print('Table with Articles and Topic: ')\n",
    "print(df)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0612d6a0-705a-4a3b-a3a8-bbe12005311f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"Virus\" (weight:  0.097\n",
      "- \"Biden\" (weight:  0.097\n",
      "- \"Missing\" (weight:  0.057\n",
      "- \"Joins\" (weight:  0.057\n",
      "- \"Where\" (weight:  0.057\n",
      "- \"Roger\" (weight:  0.057\n",
      "- \"Stands\" (weight:  0.057\n",
      "- \"Federer\" (weight:  0.057\n",
      "- \"Plan\" (weight:  0.057\n",
      "- \"Open\" (weight:  0.057\n",
      "\n",
      "Topic 1:\n",
      "- \"Nadal\" (weight:  0.072\n",
      "- \"Rafael\" (weight:  0.072\n",
      "- \"Open\" (weight:  0.072\n",
      "- \"Is\" (weight:  0.071\n",
      "- \"Australian\" (weight:  0.071\n",
      "- \"Out\" (weight:  0.071\n",
      "- \"Reality\" (weight:  0.071\n",
      "- \"Meet\" (weight:  0.071\n",
      "- \"Plans\" (weight:  0.071\n",
      "- \"Biden\" (weight:  0.070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Top Terms for Each Topic:')\n",
    "for idx, topic in lda_model.print_topics():\n",
    "\tprint(f'Topic {idx}:')\n",
    "\tterms = [term.strip() for term in topic.split('+')]\n",
    "\tfor term in terms:\n",
    "\t\tweight, word = term.split('*')\n",
    "\t\tprint(f'- {word.strip()} (weight:  {weight.strip()}')\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a1e5a-1f97-4d73-b27e-28939925e54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
