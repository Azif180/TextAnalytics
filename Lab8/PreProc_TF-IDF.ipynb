{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42fcdcd5-0f4a-43d0-adb4-0db54b07750c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\azif\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\azif\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\azif\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\azif\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\azif\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\azif\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e73576c-d795-4ffc-9ece-1ae074579caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20a9c7a4-3ed1-4cfe-9a31-141a0316bd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Azif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stem the words\n",
    "    ps = PorterStemmer()\n",
    "    words = [ps.stem(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cbdf553-476f-457d-987c-9e696698c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\"I love playing football on the weekends\",\n",
    " \"I enjoy hiking and camping in the mountains\",\n",
    " \"I like to read books and watch movies\",\n",
    " \"I prefer playing video games over sports\",\n",
    " \"I love listening to music and going to concerts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e6cfbc4-beb5-4da7-9ab9-2d1609e8a36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess each document in the dataset\n",
    "processed_dataset = [preprocess_text(doc) for doc in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9259057-8729-4c63-9db2-015eed413f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            0\n",
      "I enjoy hiking and camping in the mountains                        1\n",
      "I like to read books and watch movies                              0\n",
      "I prefer playing video games over sports                           0\n",
      "I love listening to music and going to concerts                    0\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0:\n",
      " love\n",
      " play\n",
      " footbal\n",
      " weekend\n",
      " go\n",
      " sport\n",
      " music\n",
      " concert\n",
      " video\n",
      " game\n",
      "\n",
      "Cluster 1:\n",
      " camp\n",
      " enjoy\n",
      " hike\n",
      " mountain\n",
      " weekend\n",
      " listen\n",
      " concert\n",
      " footbal\n",
      " game\n",
      " go\n",
      "\n",
      "Purity: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Azif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Preprocessing function\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stem the words\n",
    "    ps = PorterStemmer()\n",
    "    words = [ps.stem(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "# Step 2: Create the documents\n",
    "dataset = [\"I love playing football on the weekends\",\n",
    "           \"I enjoy hiking and camping in the mountains\",\n",
    "           \"I like to read books and watch movies\",\n",
    "           \"I prefer playing video games over sports\",\n",
    "           \"I love listening to music and going to concerts\"]\n",
    "\n",
    "# Preprocess each document\n",
    "processed_dataset = [preprocess_text(doc) for doc in dataset]\n",
    "\n",
    "# Step 3: Vectorize the preprocessed dataset\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(processed_dataset)\n",
    "\n",
    "# Step 4: Perform clustering\n",
    "k = 2 # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display the document and its predicted cluster in a table\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Print top terms per cluster\n",
    "print(\"\\nTop terms per cluster:\")\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(k):\n",
    "    print(f\"Cluster {i}:\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f' {terms[ind]}')\n",
    "    print()\n",
    "\n",
    "# Step 5: Evaluate results (Purity)\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa0b43-2513-46d8-9dea-017c734c6523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
